# ğŸ“Š Value at Risk (VaR) ETL Pipeline â€“ Databricks Asset Bundle

## ğŸš€ Overview

This project implements a production-style ETL pipeline to compute **Historical Value at Risk (VaR)** for equity instruments using a **Medallion Architecture (Bronze â†’ Silver â†’ Gold)** on Databricks.

The pipeline is fully automated using:

- Databricks Asset Bundles  
- GitHub Actions (CI/CD)  
- Delta Lake  
- PySpark  
- Unit testing with pytest  
- Linting via ruff (uv-based environment)  

This project simulates how a quantitative risk team would structure a scalable, environment-aware, production-ready data platform.

---

## ğŸ— Architecture

The system follows a Medallion Architecture:

### ğŸ”¹ Bronze Layer
- Ingests raw stock data (Auto Loader / external source)
- Stores raw JSON or structured market data
- Append-only Delta table

### ğŸ”¹ Silver Layer
- Cleans and standardizes data
- Computes log returns
- Handles nulls and data validation
- Stores curated return dataset

### ğŸ”¹ Gold Layer
- Computes Historical VaR
- Parameterized by:
  - `as_of_date`
  - `look_back`
  - `alpha`
- Supports upsert / merge logic
- Produces production-ready risk output table

---

## ğŸ“ VaR Methodology

This project implements **Historical Simulation VaR**:

VaR(Î±) = - Quantile(1 - Î±) of returns

Parameters:

| Parameter     | Description                                   |
|--------------|-----------------------------------------------|
| `alpha`      | Confidence level (e.g., 0.95)                 |
| `look_back`  | Rolling window size (e.g., 252 trading days)  |
| `as_of_date` | Evaluation date                               |

The design supports backfilling and multi-day computation.

---

## âš™ï¸ Project Structure

```text
var_etl/
â”‚
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ bronze_ingestion.py
â”‚   â”œâ”€â”€ silver_returns.py
â”‚   â””â”€â”€ gold_var_by_ticker.py
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_returns.py
â”‚   â””â”€â”€ test_var.py
â”‚
â”œâ”€â”€ databricks.yml
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ uv.lock
â””â”€â”€ .github/workflows/
 ```

---

## ğŸ”„ CI/CD Workflow

### ğŸ§ª On Feature Branch Push
- Install dependencies using uv
- Run linting (ruff)
- Run unit tests (pytest)
- Deploy to **dev workspace**

### ğŸš€ On Merge to Main
- Validate Databricks bundle
- Deploy to **prod workspace**

This mimics real-world Dev â†’ Prod promotion workflows.

---

## ğŸ§ª Testing Strategy

Unit tests validate:

- Log return calculation
- VaR quantile correctness
- Edge cases (null values, insufficient window)
- Deterministic Spark transformations

All transformations are implemented as pure Python functions, making them testable outside notebook context.

---

## ğŸ” Environment Management

Environment separation is handled via:

- Databricks Bundle targets
- GitHub Secrets:
  - `DATABRICKS_TOKEN`
  - `DATABRICKS_HOST`
- Environment-aware deployment (`dev`, `prod`)

No credentials are stored in code.

---

## ğŸ›  Tech Stack

- Databricks  
- Delta Lake  
- PySpark  
- Python 3.11  
- uv (package manager)  
- pytest  
- ruff  
- GitHub Actions  
- Databricks Asset Bundles  

---

## ğŸ“Š Example Output

| symbol | as_of_date | alpha | look_back | var     |
|--------|------------|-------|-----------|---------|
| AAPL   | 2026-02-13 | 0.95  | 252       | -0.0213 |

---

## ğŸ¯ Design Principles

- Clean separation of layers
- Idempotent transformations
- Parameterized execution
- Production-grade CI/CD
- Environment isolation
- Testable Spark transformations
- Extensible architecture

---



